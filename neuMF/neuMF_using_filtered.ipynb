{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyMw4pf9xjXI8cI/CqoNMblf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eUuriR2HcKm3","executionInfo":{"status":"ok","timestamp":1701948334194,"user_tz":-540,"elapsed":27691,"user":{"displayName":"나주영","userId":"11176719384042417975"}},"outputId":"5106610e-ea38-48c6-9a69-71a7202bbf6c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import json\n","import pickle"],"metadata":{"id":"v3glkGrGcML4","executionInfo":{"status":"ok","timestamp":1701948335526,"user_tz":-540,"elapsed":1336,"user":{"displayName":"나주영","userId":"11176719384042417975"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/집교 2_Team P/user-track-listen_count_filtered5.csv')"],"metadata":{"id":"kRDZDyXPcOp9","executionInfo":{"status":"ok","timestamp":1701948337276,"user_tz":-540,"elapsed":1753,"user":{"displayName":"나주영","userId":"11176719384042417975"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["df_embedding = pd.read_pickle('/content/drive/MyDrive/집교 2_Team P/lyrics_Embedding/all_embeddings_combined.pkl')"],"metadata":{"id":"smE1s0RFcO7Y","executionInfo":{"status":"ok","timestamp":1701948339290,"user_tz":-540,"elapsed":2018,"user":{"displayName":"나주영","userId":"11176719384042417975"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Pickle 파일 읽기\n","with open('/content/drive/MyDrive/집교 2_Team P/lyrics_Embedding/all_embeddings_combined.pkl', 'rb') as file:\n","    data = pickle.load(file)\n","\n","# DataFrame으로 변환\n","df_embedding = pd.DataFrame(data, columns=['embedding', 'track_id'])\n","\n","# track_id를 정수로 변환 (필요하다면)\n","df_embedding['track_id'] = df_embedding['track_id'].astype(int)\n","\n","# 'embedding' 열을 768차원의 각 차원으로 나누기\n","# df_embedding[['embedding_{}'.format(i) for i in range(768)]] = pd.DataFrame(df_embedding['embedding'].tolist(), index=df_embedding.index)\n","\n","# 'embedding' 열 삭제\n","# df_embedding = df_embedding.drop(['embedding'], axis=1)\n","\n","# DataFrame 확인\n","print(df_embedding.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"81ljehBxekam","executionInfo":{"status":"ok","timestamp":1701948339290,"user_tz":-540,"elapsed":5,"user":{"displayName":"나주영","userId":"11176719384042417975"}},"outputId":"a657fd36-ceb8-4ad9-aa31-01e9208ecaff"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["                                           embedding  track_id\n","0  [0.012072664, 0.17292306, 0.0061238254, 0.0707...         2\n","1  [-0.17554894, 0.24209566, 0.4195969, -0.185033...         8\n","2  [-0.096951924, 0.0034472912, 0.005701333, 0.01...      1524\n","3  [-0.21775067, 0.244962, 0.24090661, 0.1647732,...      1785\n","4  [-0.069424234, -0.016805744, 0.21406727, -0.27...      1787\n"]}]},{"cell_type":"code","source":["df = pd.merge(df, df_embedding, on=['track_id'], how='left')"],"metadata":{"id":"DfpRw-VmeiN_","executionInfo":{"status":"ok","timestamp":1701948340501,"user_tz":-540,"elapsed":1214,"user":{"displayName":"나주영","userId":"11176719384042417975"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4G-Q5aiNcHSA","executionInfo":{"status":"ok","timestamp":1701949786965,"user_tz":-540,"elapsed":2483,"user":{"displayName":"나주영","userId":"11176719384042417975"}},"outputId":"48a139f6-6aed-4873-bc36-8d39fd2936d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["num_users: 23761\n","num_items: 28378\n","Using device: cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.metrics import mean_squared_error\n","# 가상의 데이터 프레임 생성 (예시)\n","# ...\n","\n","# Label Encoding\n","user_encoder = LabelEncoder()\n","item_encoder = LabelEncoder()\n","\n","df['user_id'] = user_encoder.fit_transform(df['user_id'])\n","df['track_id'] = item_encoder.fit_transform(df['track_id'])\n","df.loc[df['listen_count_bin'] == '10~2704', 'listen_count_bin'] = 10\n","df['listen_count_bin'] = df['listen_count_bin'].astype(int)\n","\n","# PyTorch DataLoader에 맞게 데이터 변환\n","def df_to_tensor(dataset):\n","    users = torch.tensor(dataset['user_id'].values, dtype=torch.long)\n","    items = torch.tensor(dataset['track_id'].values, dtype=torch.long)\n","    ratings = torch.tensor(dataset['listen_count_bin'].values, dtype=torch.float)\n","    return users, items, ratings\n","\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","train_users, train_items, train_ratings = df_to_tensor(train_df)\n","test_users, test_items, test_ratings = df_to_tensor(test_df)\n","\n","train_data = TensorDataset(train_users, train_items, train_ratings)\n","test_data = TensorDataset(test_users, test_items, test_ratings)\n","\n","train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n","test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n","\n","class NeuMF(nn.Module):\n","    def __init__(self, num_users, num_items, embedding_size, mlp_hidden_size):\n","        super(NeuMF, self).__init__()\n","        # Matrix Factorization\n","        self.user_embedding_mf = nn.Embedding(num_users, embedding_size)\n","        self.item_embedding_mf = nn.Embedding(num_items, embedding_size)\n","        # Multi-Layer Perceptron\n","        self.user_embedding_mlp = nn.Embedding(num_users, mlp_hidden_size)\n","        self.item_embedding_mlp = nn.Embedding(num_items, mlp_hidden_size)\n","        self.mlp_layers = nn.Sequential(\n","            nn.Linear(2 * mlp_hidden_size, mlp_hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(mlp_hidden_size, int(mlp_hidden_size/2)),\n","            nn.ReLU(),\n","            nn.Linear(int(mlp_hidden_size/2), int(mlp_hidden_size/4)),\n","        )\n","        # Final Layer\n","        self.final_layer = nn.Linear(int(mlp_hidden_size/4+embedding_size), 1)\n","\n","    def forward(self, user, item):\n","        # Matrix Factorization\n","        user_embedding_mf = self.user_embedding_mf(user)\n","        item_embedding_mf = self.item_embedding_mf(item)\n","        mf_output = torch.mul(user_embedding_mf, item_embedding_mf)\n","\n","        # Multi-Layer Perceptron\n","        user_embedding_mlp = self.user_embedding_mlp(user)\n","        item_embedding_mlp = self.item_embedding_mlp(item)\n","        mlp_input = torch.cat((user_embedding_mlp, item_embedding_mlp), dim=1)\n","        mlp_output = self.mlp_layers(mlp_input)\n","        # Concatenate MF and MLP outputs\n","        final_input = torch.cat((mf_output, mlp_output), dim=1)\n","\n","        # Final prediction\n","        prediction = self.final_layer(final_input)\n","        return prediction.view(-1)\n","\n","\n","# NeuMF 모델 초기화\n","num_users = len(user_encoder.classes_)\n","num_items = len(item_encoder.classes_)\n","print(\"num_users:\",num_users)\n","print(\"num_items:\",num_items)\n","\n","\n","# 학습 및 평가 코드는 이전과 유사\n","# ...\n","# CUDA 디바이스 설정\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","def train_(emb,mlp,n_epoch):\n","  embedding_size = emb\n","  mlp_hidden_size = mlp\n","\n","  neumf_model = NeuMF(num_users, num_items, embedding_size, mlp_hidden_size)\n","\n","  # 손실 함수 및 최적화 함수 정의\n","  criterion = nn.MSELoss()\n","  optimizer = optim.Adam(neumf_model.parameters(), lr=0.001)\n","  # NCF 모델 정의 및 GPU로 이동\n","  # model = NCF(num_users=len(user_encoder.classes_), num_items=len(item_encoder.classes_), embedding_size=embedding_size)\n","  neumf_model.to(device)\n","  criterion = nn.MSELoss()\n","  optimizer = optim.Adam(neumf_model.parameters(), lr=0.001)\n","  # tqdm을 사용하여 학습 및 테스트 진행 상황 확인\n","  num_epochs = n_epoch\n","  for epoch in range(num_epochs):\n","      neumf_model.train()\n","      total_loss = 0\n","      for user, item, rating in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n","          optimizer.zero_grad()\n","          user, item, rating = user.to(device), item.to(device), rating.to(device)  # GPU로 이동\n","          output = neumf_model(user, item)\n","          loss = criterion(output, rating.unsqueeze(1))\n","          loss.backward()\n","          optimizer.step()\n","          total_loss += loss.item()\n","\n","      avg_loss = total_loss / len(train_loader)\n","      print(f'Epoch {epoch+1}/{num_epochs}, Avg. Loss: {avg_loss:.4f}')\n","\n","      # 각 에폭이 끝날 때마다 테스트 데이터에 대한 예측 수행\n","      neumf_model.eval()\n","      all_predictions = []\n","      with torch.no_grad():\n","          for user, item, _ in tqdm(test_loader, desc=f'Testing Epoch {epoch+1}'):\n","              user, item = user.to(device), item.to(device)  # GPU로 이동\n","              output = neumf_model(user, item)\n","              all_predictions.append(output)\n","\n","      # RMSE 계산\n","      predictions = torch.cat(all_predictions).squeeze().cpu().numpy()  # CPU로 이동 후 numpy로 변환\n","      rmse = np.sqrt(mean_squared_error(test_df['listen_count_bin'].values, predictions))\n","      print(f'Epoch {epoch+1}/{num_epochs}, RMSE on test set: {rmse}')"]},{"cell_type":"code","source":["train_(64,256,10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"KUgHfnO3poiF","executionInfo":{"status":"error","timestamp":1701950585144,"user_tz":-540,"elapsed":798186,"user":{"displayName":"나주영","userId":"11176719384042417975"}},"outputId":"a124e76f-9f5a-47a0-dd64-46b315af8edf"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10:   0%|          | 0/14516 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 1/10: 100%|█████████▉| 14501/14516 [01:37<00:00, 153.78it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([168, 1])) that is different to the input size (torch.Size([168])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 1/10: 100%|██████████| 14516/14516 [01:38<00:00, 147.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Avg. Loss: 1.6497\n"]},{"output_type":"stream","name":"stderr","text":["Testing Epoch 1: 100%|██████████| 3629/3629 [00:12<00:00, 282.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, RMSE on test set: 1.2815246719344136\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10:   0%|          | 0/14516 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 2/10: 100%|█████████▉| 14510/14516 [01:39<00:00, 147.11it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([168, 1])) that is different to the input size (torch.Size([168])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 2/10: 100%|██████████| 14516/14516 [01:39<00:00, 145.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10, Avg. Loss: 1.6430\n"]},{"output_type":"stream","name":"stderr","text":["Testing Epoch 2: 100%|██████████| 3629/3629 [00:12<00:00, 280.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10, RMSE on test set: 1.2812149953172507\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10:   0%|          | 0/14516 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 3/10: 100%|█████████▉| 14504/14516 [01:39<00:00, 144.64it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([168, 1])) that is different to the input size (torch.Size([168])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 3/10: 100%|██████████| 14516/14516 [01:39<00:00, 145.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/10, Avg. Loss: 1.6429\n"]},{"output_type":"stream","name":"stderr","text":["Testing Epoch 3: 100%|██████████| 3629/3629 [00:12<00:00, 283.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/10, RMSE on test set: 1.281231898226455\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10:   0%|          | 0/14516 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 4/10: 100%|█████████▉| 14506/14516 [01:39<00:00, 146.33it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([168, 1])) that is different to the input size (torch.Size([168])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 4/10: 100%|██████████| 14516/14516 [01:40<00:00, 144.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/10, Avg. Loss: 1.6429\n"]},{"output_type":"stream","name":"stderr","text":["Testing Epoch 4: 100%|██████████| 3629/3629 [00:12<00:00, 282.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/10, RMSE on test set: 1.2813127533367867\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10:   0%|          | 0/14516 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 5/10: 100%|█████████▉| 14505/14516 [01:39<00:00, 151.80it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([168, 1])) that is different to the input size (torch.Size([168])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 5/10: 100%|██████████| 14516/14516 [01:39<00:00, 145.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/10, Avg. Loss: 1.6429\n"]},{"output_type":"stream","name":"stderr","text":["Testing Epoch 5: 100%|██████████| 3629/3629 [00:12<00:00, 282.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/10, RMSE on test set: 1.2817298509244697\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10:   0%|          | 0/14516 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 6/10: 100%|█████████▉| 14504/14516 [01:39<00:00, 146.24it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([168, 1])) that is different to the input size (torch.Size([168])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 6/10: 100%|██████████| 14516/14516 [01:39<00:00, 145.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/10, Avg. Loss: 1.6428\n"]},{"output_type":"stream","name":"stderr","text":["Testing Epoch 6: 100%|██████████| 3629/3629 [00:12<00:00, 281.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/10, RMSE on test set: 1.2819973795934825\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10:   0%|          | 0/14516 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 7/10: 100%|█████████▉| 14502/14516 [01:39<00:00, 149.36it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([168, 1])) that is different to the input size (torch.Size([168])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 7/10: 100%|██████████| 14516/14516 [01:39<00:00, 145.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/10, Avg. Loss: 1.6427\n"]},{"output_type":"stream","name":"stderr","text":["Testing Epoch 7: 100%|██████████| 3629/3629 [00:13<00:00, 278.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/10, RMSE on test set: 1.2812932811738698\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10:   0%|          | 0/14516 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 8/10:  10%|█         | 1490/14516 [00:10<01:29, 145.69it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-12ec3f9917a4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-4e130fcdc9bb>\u001b[0m in \u001b[0;36mtrain_\u001b[0;34m(emb, mlp, n_epoch)\u001b[0m\n\u001b[1;32m    114\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m           \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m       \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["train_(128,128,10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":829},"id":"zOxG3_uVB2Ui","executionInfo":{"status":"error","timestamp":1701949736692,"user_tz":-540,"elapsed":372825,"user":{"displayName":"나주영","userId":"11176719384042417975"}},"outputId":"897cac14-660e-4e47-c0b7-9d58e189ba35"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10:   0%|          | 0/14516 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 1/10: 100%|█████████▉| 14512/14516 [01:34<00:00, 159.57it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([168, 1])) that is different to the input size (torch.Size([168])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 1/10: 100%|██████████| 14516/14516 [01:34<00:00, 153.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Avg. Loss: 1.6548\n"]},{"output_type":"stream","name":"stderr","text":["Testing Epoch 1: 100%|██████████| 3629/3629 [00:12<00:00, 286.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, RMSE on test set: 1.2814876983654866\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10:   0%|          | 0/14516 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 2/10: 100%|█████████▉| 14509/14516 [01:36<00:00, 143.04it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([168, 1])) that is different to the input size (torch.Size([168])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 2/10: 100%|██████████| 14516/14516 [01:37<00:00, 149.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10, Avg. Loss: 1.6432\n"]},{"output_type":"stream","name":"stderr","text":["Testing Epoch 2: 100%|██████████| 3629/3629 [00:12<00:00, 286.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10, RMSE on test set: 1.2821177861044055\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10:   0%|          | 0/14516 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 3/10: 100%|█████████▉| 14504/14516 [01:36<00:00, 155.47it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([168, 1])) that is different to the input size (torch.Size([168])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 3/10: 100%|██████████| 14516/14516 [01:36<00:00, 150.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/10, Avg. Loss: 1.6431\n"]},{"output_type":"stream","name":"stderr","text":["Testing Epoch 3: 100%|██████████| 3629/3629 [00:12<00:00, 284.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/10, RMSE on test set: 1.2816247907843161\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10:   0%|          | 0/14516 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","Epoch 4/10:  47%|████▋     | 6889/14516 [00:45<00:50, 150.97it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-58db4dadc2fd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-3fda9c99368d>\u001b[0m in \u001b[0;36mtrain_\u001b[0;34m(emb, mlp, n_epoch)\u001b[0m\n\u001b[1;32m    115\u001b[0m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneumf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrating\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m           \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["train_(256,256,10)"],"metadata":{"id":"xTRcgzNvB5DI","executionInfo":{"status":"aborted","timestamp":1701949736695,"user_tz":-540,"elapsed":17,"user":{"displayName":"나주영","userId":"11176719384042417975"}}},"execution_count":null,"outputs":[]}]}